{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Zadanie 1\n",
    "\n",
    "\"\"\"\n",
    "Masz następujące dane:\n",
    "y_true = [10, 20, 30, 40, 50]\n",
    "y_pred = [12, 18, 33, 37, 52]\n",
    "\n",
    "Wymagania:\n",
    "Oblicz MSE ręcznie (bez bibliotek)\n",
    "Oblicz MAE ręcznie\n",
    "Zweryfikuj wyniki używając sklearn.metrics\n",
    "\n",
    "Oczekiwany rezultat:\n",
    "Wartości MSE i MAE\n",
    "Porównanie z sklearn\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "y_true = np.array([10, 20, 30, 40, 50])\n",
    "y_pred = np.array([12, 18, 33, 37, 52])\n",
    "\n",
    "errors = y_true - y_pred\n",
    "squared_errors = errors ** 2\n",
    "mse = np.mean(squared_errors)\n",
    "mae = np.mean(np.abs(errors))\n",
    "\n",
    "mse_sk = mean_squared_error(y_true, y_pred)\n",
    "mae_sk = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "print(f\"\"\"\n",
    "MSE ręcznie: {mse:.2f}, MSE ze Scikit-Learn: {mse_sk:.2f}\n",
    "MAE ręcznie: {mae:.2f}, MAE ze Scikit-Learn: {mae_sk:.2f}\n",
    "\"\"\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Zadanie 2 - Wpływ outliera na MSE i MAE\n",
    "\n",
    "\"\"\"\n",
    "Dataset: Stwórz wektor 20 losowych wartości z rozkładu normalnego (mean=50, std=10).\n",
    "\n",
    "Wymagania:\n",
    "Oblicz MSE i MAE dla tych danych (predykcja = średnia)\n",
    "Dodaj jeden outlier (wartość 200)\n",
    "Oblicz ponownie MSE i MAE\n",
    "Porównaj wzrost obu metryk\n",
    "\n",
    "Oczekiwany rezultat:\n",
    "Tabela z metrykami przed i po dodaniu outliera\n",
    "Wykres pokazujący dane z outlierem\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=50, scale=10, size=20)\n",
    "y_pred = np.mean(data)"
   ],
   "id": "2b95ab882f5a986c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Zadanie 3 - Log Loss dla klasyfikacji\n",
    "\n",
    "\"\"\"\n",
    "Masz następujące predykcje prawdopodobieństwa:\n",
    "y_true = [1, 0, 1, 1, 0]\n",
    "Model A: [0.9, 0.2, 0.8, 0.7, 0.3]\n",
    "Model B: [0.6, 0.4, 0.55, 0.65, 0.45]\n",
    "\n",
    "Wymagania:\n",
    "Oblicz Log Loss dla obu modeli\n",
    "Określ, który model jest lepszy\n",
    "Uzasadnij wynik\n",
    "\n",
    "Oczekiwany rezultat:\n",
    "Wartości Log Loss\n",
    "Interpretacja wyników\n",
    "\"\"\""
   ],
   "id": "567421ab862b103",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Zadanie 4 - Gradient Descent dla f(x) = (x-3)²\n",
    "\n",
    "\"\"\"\n",
    "Zaimplementuj Gradient Descent dla funkcji f(x) = (x-3)².\n",
    "\n",
    "Wymagania:\n",
    "Napisz funkcję f(x) i jej gradient\n",
    "Uruchom GD z punktu startowego x=10\n",
    "Learning rate = 0.1, 50 iteracji\n",
    "Wyświetl trajektorię na wykresie\n",
    "\n",
    "Oczekiwany rezultat:\n",
    "Wykres funkcji z zaznaczoną trajektorią GD\n",
    "\"\"\""
   ],
   "id": "89b96ddccd7249ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Zadanie 5 - Porównanie Learning Rates\n",
    "\n",
    "\"\"\"\n",
    "Dla funkcji f(x) = x² uruchom Gradient Descent z różnymi learning rates.\n",
    "\n",
    "Wymagania:\n",
    "Przetestuj: η = 0.001, 0.01, 0.1, 0.5, 1.0, 1.5\n",
    "Punkt startowy: x = 5\n",
    "30 iteracji\n",
    "Określ, które η działa najlepiej\n",
    "\n",
    "Oczekiwany rezultat:\n",
    "Wykres krzywych uczenia dla każdego η\n",
    "Wnioski o optymalnym learning rate\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def function(x):\n",
    "    return x**2\n",
    "def gradient(x):\n",
    "    return 2*x\n",
    "def run_gd(start_x, learning_rate, num_iterations):\n",
    "    x = start_x\n",
    "    history = [x]\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        x = x - learning_rate * gradient(x)\n",
    "        history.append(x)\n",
    "\n",
    "    return history\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0, 1.5]\n",
    "start_x = 5.0\n",
    "num_iterations = 30\n",
    "\n",
    "histories = {}\n",
    "final_values = {}\n",
    "\n",
    "# uruchomienie Gradient Descent dla każdego learning rate\n",
    "for lr in learning_rates:\n",
    "    history = run_gd(start_x, lr, num_iterations)\n",
    "    histories[lr] = history\n",
    "    final_values[lr] = abs(history[-1])  # odległość od minimum (0)\n",
    "\n",
    "# rysowanie wykresu krzywych uczenia\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for lr, history in histories.items():\n",
    "    values = [function(x) for x in history]\n",
    "    plt.plot(values, marker='o', linewidth=2, label=f\"η = {lr}\")\n",
    "\n",
    "plt.yscale(\"log\")  # skala logarytmiczna żeby wszystkie krzywe były widoczne\n",
    "plt.title(\"Krzywe uczenia Gradient Descent (wartość funkcji f(x))\")\n",
    "plt.xlabel(\"Iteracja\")\n",
    "plt.ylabel(\"f(x) = x² (skala log)\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# dla porównania wykres bez skali logarytmicznej na osi y\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for lr, history in histories.items():\n",
    "    values = [function(x) for x in history]\n",
    "    plt.plot(values, marker='o', linewidth=2, label=f\"η = {lr}\")\n",
    "\n",
    "plt.title(\"Krzywe uczenia Gradient Descent (wartość funkcji f(x))\")\n",
    "plt.xlabel(\"Iteracja\")\n",
    "plt.ylabel(\"f(x) = x² (skala log)\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# wybór learning rate\n",
    "best_lr = min(final_values, key=final_values.get)\n",
    "\n",
    "print(\"=== PODSUMOWANIE ===\")\n",
    "for lr in learning_rates:\n",
    "    print(f\"η = {lr:<4} -> końcowe |x| = {final_values[lr]:.6f}\")\n",
    "\n",
    "print(f\"\\nOptymalny learning rate: η = {best_lr}\")\n",
    "\n",
    "# printowanie wniosków\n",
    "print(\"\\n=== WNIOSKI ===\")\n",
    "\n",
    "if best_lr < 0.01:\n",
    "    print(\"Bardzo mały learning rate – zbieżność jest stabilna, ale bardzo wolna.\")\n",
    "elif best_lr <= 0.5:\n",
    "    print(\"Średni learning rate – szybka i stabilna zbieżność do minimum (najlepszy wybór).\")\n",
    "elif best_lr <= 1.0:\n",
    "    print(\"Duży learning rate – może powodować oscylacje wokół minimum.\")\n",
    "else:\n",
    "    print(\"Zbyt duży learning rate – algorytm rozbiega się i nie znajduje minimum.\")\n"
   ],
   "id": "ac0d70bf5525ab06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Zadanie 9 - Implementacja Gradient Descent dla regresji liniowej\n",
    "\n",
    "\"\"\"\n",
    "Zaimplementuj od zera Gradient Descent dla regresji liniowej y = wx + b.\n",
    "Dataset: Wygeneruj dane make_regression(n_samples=200, n_features=1, noise=20)\n",
    "\n",
    "Wymagania:\n",
    "Napisz funkcje: predict, compute_mse, compute_gradients\n",
    "Zaimplementuj pętlę treningu\n",
    "Porównaj z sklearn LinearRegression\n",
    "\n",
    "Oczekiwany rezultat:\n",
    "Wykres danych z linią regresji\n",
    "Krzywa uczenia (loss vs iteracje)\n",
    "Porównanie parametrów z sklearn\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "np.random.seed(42)\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=20)\n",
    "X = X.flatten()\n",
    "\n",
    "print(f\"Dane: {len(X)} obserwacji\")\n",
    "print(f\"X: zakres [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"y: zakres [{y.min():.2f}, {y.max():.2f}]\")\n",
    "\n",
    "def predict(X, w ,b):\n",
    "    return w * X + b\n",
    "\n",
    "def compute_mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def compute_gradients(X, y, w, b):\n",
    "    n = len(y)\n",
    "    y_pred = predict(X, w, b)\n",
    "    error = y - y_pred\n",
    "\n",
    "    dw = (-2/n) * np.sum(error * X)\n",
    "    db = (-2/n) * np.sum(error)\n",
    "\n",
    "    return dw, db\n",
    "\n",
    "def gradient_descent_linear(X, y, learning_rate=0.01, num_iterations=1000):\n",
    "    w = np.random.randn()\n",
    "    b = np.random.randn()\n",
    "\n",
    "    history = {\n",
    "        \"w\": [w], \"b\": [b],\n",
    "        \"loss\": [compute_mse(y, predict(X, w, b))]\n",
    "    }\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        dw, db = compute_gradients(X, y, w, b)\n",
    "\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        loss = compute_mse(y, predict(X, w, b))\n",
    "        history['w'].append(w)\n",
    "        history['b'].append(b)\n",
    "        history['loss'].append(loss)\n",
    "\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"Iteracja {i+1}: w = {w:.4f}, b = {b:.4f}, MSE = {loss:.4f}\")\n",
    "\n",
    "    return w, b, history\n",
    "\n",
    "print(\"\\n=== Trening Gradient Descent ===\\n\")\n",
    "w_final, b_final, history = gradient_descent_linear(X, y, learning_rate=0.01, num_iterations=1000)\n",
    "print(f\"\\nWyuczone parametry: w = {w_final:.4f}, b = {b_final:.4f}\")\n",
    "print(f\"Końcowy MSE: {history['loss'][-1]:.4f}\")\n",
    "# Wizualizacja\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# 1. Dane i linia regresji\n",
    "axes[0].scatter(X, y, alpha=0.6, label='Dane')\n",
    "x_line = np.linspace(X.min(), X.max(), 100)\n",
    "axes[0].plot(x_line, predict(x_line, w_final, b_final), 'r-',\n",
    "    linewidth=2, label=f'y = {w_final:.2f}x + {b_final:.2f}')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Regresja liniowa (Gradient Descent)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Krzywa uczenia (Loss)\n",
    "axes[1].plot(history['loss'], 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('Iteracja')\n",
    "axes[1].set_ylabel('MSE (Loss)')\n",
    "axes[1].set_title('Krzywa uczenia')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Ewolucja parametrów\n",
    "axes[2].plot(history['w'], label='w (waga)', linewidth=2)\n",
    "axes[2].plot(history['b'], label='b (bias)', linewidth=2)\n",
    "axes[2].set_xlabel('Iteracja')\n",
    "axes[2].set_ylabel('Wartość parametru')\n",
    "axes[2].set_title('Ewolucja parametrów')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Porównanie ze sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X.reshape(-1, 1), y)\n",
    "print(f\"\\nSklearn LinearRegression: w = {lr.coef_[0]:.4f}, b ={lr.intercept_:.4f}\")"
   ],
   "id": "1de91f8cf9f2147a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "84ca1afae5bb56b3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
